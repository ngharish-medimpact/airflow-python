apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-scheduler
  namespace: pdp-dev
  labels:
    tier: airflow
    component: scheduler
    executor: "CeleryExecutor"
spec:
  replicas: 1
  selector:
    matchLabels:
      tier: airflow
      component: scheduler
  template:
    metadata:
      labels:
        tier: airflow
        component: scheduler
      # annotations:
      #   checksum/configmap: 5873919b552e6912c3de9638c4c348f95c27633e3a891787d559a441a3910ffb
      #   checksum/secrets: ~
    spec:
      restartPolicy: Always
      terminationGracePeriodSeconds: 10
      serviceAccountName: "airflow-scheduler"
      securityContext:
        runAsUser: 50000
        fsGroup: 0
      initContainers:
        - name: dag-fetcher
          image: docker-local.artifactory.medimpact.com/com/medimpact/pdp/medgen-dags-python:0.0.6
          imagePullPolicy: Always
          command:
            - "rsync"
            - "-av"
            - "--no-times"
            - "--no-perms"
            - "/source_files/dags/"
            - "/dags/"
          volumeMounts:
            - name: dags-volume
              mountPath: "/dags"
        - name: wait-for-airflow-migrations
          resources:
            requests:
              cpu: "500m"
              memory: "1Gi"
            limits:
              cpu: "1"
              memory: "2Gi"
          image: docker-local.artifactory.medimpact.com/dockerhub/apache/airflow:slim-3.0.2
          imagePullPolicy: Always
          command:
            - "/bin/bash"
            - "-c"
            - |
              exec /entrypoint airflow db check-migrations --migration-wait-timeout=60
          env:
            - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
              value: sqlite:////opt/airflow/airflow.db
      containers:
        - name: scheduler
          image: docker-local.artifactory.medimpact.com/dockerhub/apache/airflow:slim-3.0.2
          imagePullPolicy: Always
          args:
            - "bash"
            - "-c"
            - "exec airflow scheduler"
          resources:
            {}
          volumeMounts:
            - name: dags-volume
              mountPath: "/opt/airflow/dags"
            - name: config
              mountPath: /opt/airflow/pod_templates/pod_template_file.yaml
              subPath: pod_template_file.yaml
              readOnly: true
            - name: logs
              mountPath: "/opt/airflow/logs"
            - name: config
              mountPath: "/opt/airflow/airflow.cfg"
              subPath: airflow.cfg
              readOnly: true
            - name: config
              mountPath: "/opt/airflow/config/airflow_local_settings.py"
              subPath: airflow_local_settings.py
              readOnly: true
          env:
            - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
              value: sqlite:////opt/airflow/airflow.db
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-redis-password
                  key: password
            - name: AIRFLOW__CELERY__RESULT_BACKEND
              value: "db+redis://:$(REDIS_PASSWORD)@airflow-redis:6379/1"
            - name: AIRFLOW__CELERY__BROKER_URL
              valueFrom:
                secretKeyRef:
                  name: airflow-broker-url
                  key: connection
          livenessProbe:
            initialDelaySeconds: 10
            timeoutSeconds: 20
            failureThreshold: 5
            periodSeconds: 60
            exec:
              command:
                - sh
                - -c
                - |
                  CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR exec /entrypoint \
                  airflow jobs check --local --job-type SchedulerJob
          startupProbe:
            initialDelaySeconds: 15
            timeoutSeconds: 20
            failureThreshold: 10
            periodSeconds: 60
            exec:
              command:
                - sh
                - -c
                - |
                  CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR exec /entrypoint \
                  airflow jobs check --local --job-type SchedulerJob
        - name: scheduler-log-groomer
          image: docker-local.artifactory.medimpact.com/dockerhub/apache/airflow:slim-3.0.2
          imagePullPolicy: Always
          resources:
            {}
          args:
            - "bash"
            - "-c"
            - |
              sleep 60
              exec \
              airflow jobs check --local --job-type SchedulerJob --hostname $(hostname)
          volumeMounts:
            - name: logs
              mountPath: "/opt/airflow/logs"
            - name: config
              mountPath: "/opt/airflow/airflow.cfg"
              subPath: airflow.cfg
              readOnly: true
            - name: config
              mountPath: "/opt/airflow/config/airflow_local_settings.py"
              subPath: airflow_local_settings.py
              readOnly: true
      volumes:
        - name: dags-volume
          emptyDir: {}
        - name: config
          configMap:
            name: airflow-config
        - name: logs
          emptyDir: {}